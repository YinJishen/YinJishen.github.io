<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>第五章：PyTorch实现线性回归 - Jishen Yin</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta itemprop="name" content="第五章：PyTorch实现线性回归">
<meta itemprop="description" content="PyTorch自动求梯度原理；PyTorch从零实现线性回归">
<meta itemprop="datePublished" content="2020-12-24T14:38:17-05:00" />
<meta itemprop="dateModified" content="2020-12-24T14:38:17-05:00" />
<meta itemprop="wordCount" content="151">



<meta itemprop="keywords" content="" />
<meta property="og:title" content="第五章：PyTorch实现线性回归" />
<meta property="og:description" content="PyTorch自动求梯度原理；PyTorch从零实现线性回归" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yinjishen.github.io/pytorch/v/" />
<meta property="article:published_time" content="2020-12-24T14:38:17-05:00" />
<meta property="article:modified_time" content="2020-12-24T14:38:17-05:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="第五章：PyTorch实现线性回归"/>
<meta name="twitter:description" content="PyTorch自动求梯度原理；PyTorch从零实现线性回归"/>
<link href='https://fonts.googleapis.com/css?family=Playfair+Display:700' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" type="text/css" media="screen" href="https://yinjishen.github.io/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://yinjishen.github.io/css/main.css" />

        <link id="dark-scheme" rel="stylesheet" type="text/css" href="https://yinjishen.github.io/css/dark.css" />

	<script src="https://yinjishen.github.io/js/feather.min.js"></script>
	
		<script src="https://yinjishen.github.io/js/main.js"></script>
</head>

<body>
	<div class="container wrapper">
		<div class="header">
	
	<h1 class="site-title"><a href="https://yinjishen.github.io/">Jishen Yin</a></h1>
	<div class="site-description"><nav class="nav social">
			<ul class="flat"><li><a href="https://github.com/YinJishen" title="Github"><i data-feather="github"></i></a></li></ul>
		</nav></div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="/">Home</a>
			</li>
			
			<li>
				<a href="/bio">Biostat 823</a>
			</li>
			
			<li>
				<a href="/pytorch">Pytorch</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post">
			<div class="post-header">
				
					<div class="meta">
						<div class="date">
							<span class="day">24</span>
							<span class="rest">Dec 2020</span>
						</div>
					</div>
				
				<div class="matter">
					<h1 class="title">第五章：PyTorch实现线性回归</h1>
				</div>
			</div>
					
			<div class="markdown">
				<p>此为深度学习书籍《Deep Learning with Pytorch》学习笔记整理系列第四期，因为电脑配置问题，将跳过涉及CUDA/GPU的内容</p>
<p>书籍pdf链接：https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf</p>
<p>书籍附源代码链接：https://github.com/deep-learning-with-pytorch/dlwpt-code</p>
<p>注：所有实现代码均包含以下命令，此后将省略</p>
<pre><code>%matplotlib inline
import matplotlib.pyplot as plt #导入可视化库matplotlib.pyplot
import numpy as np #导入tensor常规处理库

#Pytorch主要部件导入
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.set_printoptions(edgeitems=2) #输出长tensor时，保留首尾各2个元素
torch.manual_seed(123) #设置随机数种子
</code></pre><h1 id="第五章线性回归的从零实现">第五章：线性回归的从零实现</h1>
<h2 id="51-pytorch自动求梯度原理">5.1 PyTorch自动求梯度原理</h2>
<p>PyTorch的核心功能就在于自动求梯度。我们知道，深度学习的过程实际上就是不断地调整模型里的参数，使得损失函数尽可能的小。那么我们就会迫切地关注损失函数关于代求参数的梯度，用于更新参数。在PyTorch定义待估参数的tensor时，可以加入选项<code>requires_grad=True</code>以提供存储梯度的空间。当我们使用参数计算出损失函数值<code>loss</code>时，便可以调用<code>loss.backward()</code>，以回溯至所有和<code>loss</code>相关的带梯度tensor，此时所有<code>requires_grad=True</code>的参数所附的梯度值会自动更新。</p>
<p>这里需要注意的是，如果重复<code>backward</code>，梯度值是叠加而不是覆盖，所以在循环的时候，要仔细检查是否需要不停地将梯度清零，即<code>params.grad.zero_()</code>（带下划线表示内部操作）。</p>
<p><img src="/pytorch/5/5-1.PNG" alt="image"></p>
<!-- raw HTML omitted -->
<h2 id="52-实现线性回归">5.2 实现线性回归</h2>
<h3 id="数据">数据</h3>
<pre><code>t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]
t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]
t_c = torch.tensor(t_c)
t_u = torch.tensor(t_u)
</code></pre><h3 id="定义模型及初始化参数">定义模型及初始化参数</h3>
<pre><code>def model(t_u, w, b):
	return w * t_u + b
	
params = torch.tensor([1.0, 0.0], requires_grad=True)
</code></pre><h3 id="定义损失函数">定义损失函数</h3>
<pre><code>loss = nn.MSEloss()
</code></pre><h3 id="定义优化器">定义优化器</h3>
<pre><code>learning_rate = 1e-5 ## 学习率
optimizer = optim.SGD([params], lr=learning_rate)
</code></pre><h3 id="定义训练器">定义训练器</h3>
<pre><code>def linear_regression(n_epochs, optimizer, params, t_u, t_c):
	for epoch in range(1, n_epochs+1):
		t_p = model(t_u, *params)
		loss = loss_fn(t_p, t_c)
		
		optimizer.zero_grad()
		loss.backward()
		optimizer.step()
		
		if epoch % 500 == 0:
			print('Epoch %d, Loss %.3f' % (epoch, float(loss)))
	
	return params
</code></pre><p>我们发现，不管多么复杂的网络，核心架构都包含以下几部分</p>
<ol>
<li>模型：输入和参数如何得到输出，可借助<code>nn</code>、<code>F</code>定义</li>
<li>带梯度的参数，<code>requires_grad=True</code></li>
<li>损失函数：优化的目标函数，可借助<code>nn</code>定义</li>
<li>优化器：用什么算法优化参数，可借助<code>optim</code>定义。将待估参数输入，<code>zero_grad()</code>清零梯度，<code>step</code>更新参数</li>
</ol>

			</div>

			<div class="tags">
				
					
				
			</div></div>
	</div>
	<div class="footer wrapper">
	<nav class="nav">
		<div>2020  <a href="https://github.com/knadh/hugo-ink">Ink</a> theme on <a href="https://gohugo.io">Hugo</a></div>
	</nav>
</div>

<script>feather.replace()</script>
</body>
</html>
